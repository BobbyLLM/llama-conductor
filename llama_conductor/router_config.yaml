# router_config.yaml
# version 1.0.5 - CORRECTED

# Where to send OpenAI-compatible chat completion requests.
# Typically your llama-swap / llama.cpp OpenAI-ish endpoint.
llama_swap_url: "http://127.0.0.1:8011/v1/chat/completions"

# Router bind port (when running `python router_fastapi.py`)
port: 9000

# Model role mapping (router passes `role=`; call_model resolves to these model IDs).
roles:
  thinker: "Qwen-3-4B Hivemind"
  critic: "Phi-4-mini"
  vision: "qwen-3-4B_VISUAL"
  coder: "Qwen-3-4B Hivemind"
  second_op: "Nanbeige 3B"
  clinical: "Qwen2.5-1.5B"  # Used for >>cliniko review

# Vault is Qdrant-only; Mentats uses this kb label when searching Qdrant.
vault_kb_name: "vault"

# Filesystem KB folders.
# These are *not* Qdrant. They are folders containing raw docs + SUMM_*.md files.
kb_paths:
  c64: "C:/docs/c64"
  amiga: "C:/docs/amiga"
  dogs: "C:/docs/dogs"

# Optional RAG settings (rag.py is still using its own defaults in v1.0.3).
rag:
  qdrant_host: "localhost"
  qdrant_port: 6333
  collection: "moa_kb_docs"
  vector_name: "e5_small_v2"

# Vodka settings (overrides vodka_filter.py defaults)
vodka:
  storage_dir: "."  # empty = cwd, or set explicit path
  base_ttl_days: 3
  touch_extension_days: 5
  max_touches: 2
  debug: true
  debug_dir: "."
  n_last_messages: 10
  keep_first: true
  max_chars: 3000

# Vault chunking (for >>move to vault ingestion)
vault:
  chunk_words: 250        # Smaller chunks = better semantic matching for queries
  chunk_overlap_words: 50  # Overlap prevents losing context at chunk boundaries

# Cliniko sidecar (staged clinical note pipeline)
# >>cliniko compact: Compress 20KB+ RAW notes to ~6KB
# >>cliniko generate: LLM generates management plan draft
# >>cliniko sanitize: Deterministic cleanup (future)
# >>cliniko review: LLM polish with validation (v2.3)
cliniko:
  debug: true  # Enable debug logging for prompt/response diagnostics
  
  # Step 1: Compaction settings
  compact:
    min_chars_trigger: 1800    # Only compact if input > this
    max_out_chars: 7000         # Target output size (tested at 6287 chars from 21KB input)
    max_out_lines: 400          # Hard cap on output lines
    include_stats_header: true  # Prepend COMPACTED_STATS to user message
  
  # Step 2: LLM generation settings (UNCHANGED FROM ORIGINAL)
  generate:
    # Role key from `roles:` mapping above (NOT a model name)
    model_role: "clinical"
    max_tokens: 2000            # V2.5 SLIM optimized for 8K context window
    temperature: 0.2            # Low temp for clinical consistency
    top_p: 0.9
    # Use forward slashes to avoid YAML escape issues on Windows
    prompt_path: "C:/moa-router1.1.6 LOCAL/llama_conductor/CLINIKO-prompt.md"
  
  # Step 3: Sanitization settings (future)
  sanitize:
    enforce_must_not_miss: true
    strip_structural_claims: true
    strip_invented_metrics: true
  
  # Step 4: LLM review with validation (NEW in v2.3)
  review:
    model_role: "clinical"  # Use dedicated clinical role for review
    max_tokens: 1400
    temperature: 0.3
    top_p: 0.9

# Scratchpad sidecar (ephemeral session-grounding)
scratchpad:
  storage_dir: "."         # base dir for total_recall/session_kb
  max_entries: 12          # max captures retained per session file
  max_capture_chars: 12000 # per-capture hard clip
  top_k: 3                 # retrieval hits merged into facts block
  max_chars: 1200          # max chars contributed to facts block
  max_age_minutes: 60      # hard expiry; pruned on scratchpad access
  exhaustive_response_mode: "raw"  # raw|llm; raw bypasses LLM for "all facts/everything" intents
